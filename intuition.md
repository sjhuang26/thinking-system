I'm really having trouble expressing the main intuition. When you know there's something new, you really feel it, but then when you write it down, even though it only takes a few main metaphors, it's hard!

Stimulus vs. Simulation

The behaviorist mindset is going to get seriously attacked. People learn not by rote behaviorism (rote association A/B + rote association that "spreads" or "generalizes" to stimulus C) but rather, upon acquiring a good metaphor, sustain a mental models simulated in their visualization, and the mental models, like an organism, have several vectors of evolving into new things that can eventually become more general, right across the rationality map, and the mental models are what execute the thinking process. Except that little kids are not motivated to do this! They just get pushed around by operants. But this is not the optimal state of affairs.

Attacking subjectivism. Pleasing Paintings >> The person can be presented with 500 examples of reasonably pleasing paintings and 500 examples of "paintings" where to claim them as pleasing is irrational behavior in the context, such that the differentiation is objective between two agents. This is only explainable with a rationality map that develops in a partially objective way (e.g., considering the variation in first principles).



Same-different gradient >> paying attention to This Thing while being precommitted to One Other Thing.

More-less gradient >> an intent to spatially travel towards another more ideal object (visual comparison). OR a snowball relationship where 0, 01, 012, 0123, ...

Hierarchy gradient >> an intent to have one object act as a handle for many other details (density).


We claim the following abstractions.

All people have a CPU that spotlights in attention parts of the GPU of the cognitive map, acting as an emotional executive. When a part of the GPU is spotlighted, the computations start to run implicitly. For example, in a logic puzzle, spotlighting "A" will lead to "(NOT NOT A) == A" or etc.

The point is that the behaviorists keep studying the CPU's weird methods of rote learning (non-GPU learning), and then they say, well, a specialized form of rote learning (SMART training) "scientifically works" for IQ training, and they claim it as the CPU's relational abilities. Really? Shouldn't it make sense that when doing the logic puzzles, in fact, CPU ATTENTION is being trained and the GPU is taking over some of the cognitive operations, at a subconscious level? Rote learning only to a limited degree leads to "understanding." Radical behaviorism is some "Chinese room" construct where stimulus classes, as nice "tokens" of thought, are "pushed around" by the brain. In light of this, the nature of relations is not as the optimal mechanism to encode a whole hierarchy of variegated details but rather to LEARN the following. But once the LEARNING takes place, it is VIRTUALLY UNNECESSARY. In fact, maybe if we can instill some kind of attention-curiosity in little kids, we can get them to metaphor-ize the relations and therefore CPU work is far less necessary, other than the CPU being a mechanism of bring attention to Things. But the rationality map controls the rationality map, not the CPU with its "arbitrary applicable derived relations" controlling the rationality map. So the AADR would simply be a token-ization of the rationality map. In our system, the rationality map, not the AADR, would eventually gain control of itself. One way to do this is to visualize imagining correctly and then visualize imagining incorrectly.

The cognitive map is a fully-contained, plastic, self-shaping, and first-principle based mechanism of individualistic rationality (e.g., creative intuition) where sensory input flows in and behavior flows out (creativity).


Action items of training.


Knee-jerk reactions from the CPU's messages are a frequent occurrence in society, so psychologists address this with ACT defusion techniques.

At this point, behavior flows unhindered from the Correct-Incorrect binary (a local maxima of rationality in the moment) that the cognitive map directly maps into.

At this point, the idea of the CPU is mainly for attention. The CPU seems to be this emergent token language that is not properly grounded on rationality but is not really optimal.

The GPU contains many "layers" of various hierarchical detail densities. So the present sensory stream-of-consciousness has the highest density and some crusty old episodic memory has very little. Even recent (<1s) memory has a massive drop in density.


The GPU can do cool tricks if it mindfully pays attention.
- In some training contexts (e.g., physical-spatial-visual-visualization) it is easy to absorb sheer detail. The speed that the individual hierarchically unpacks and big details into little details (many of which are memorized and retained) is astoundingly fast compared to the declarative memory schemas.
- A strong image (e.g., the Present vision) can be deep-mapped into a strong image of a visualizations (e.g., coloring objects Red/Green/Gray) which can be also encoded in episodic memory, although the strong image very, very superlatively expotentially weakens (Ebbinghaus forgetting curve). 1000 details might turn into 100 details.


Solving RFT "puzzles" to raise IQ vs. creating intents that give the GPU more power.

What are intents?

> detail-combinatorial

> Basically, INTEND to get Thing A and Thing B and string together Random-Detail-Thing-A to Random-Detail-Thing-B, obviously, with millions of combinations available

The Guy with One Eye >> paying attention to a particular object in the scenery, temporal ping-pong.

Paying attention.

> You can threaten yourself to pay attention properly using a "10% chance" of a random quiz. This wrests power from the CPU.

temporal ping-pong

> by temporally ping-ponging between A and B at a rapid pace. This might be the origin of visualization or the "mind's eye": ping pong between Present and Synthesis of Prior Episodic Memories (including the episodic memory of the present just being encoded).

string-thin noodles augmented with a soup of metaphors

> You need a ton of metaphors so that the CPU can slowly relinquish its responsibility. A TON of metaphors. TODO.
> For instance, the use of memorizing a "NOT" sign for the NOT relation. A "LESS" sign for the LESS relation. etc.
> Since you have learned many education constructs incorrectly (through rote), you should re-learn a bunch of stuff. OR, you can create a standardized format of data exchange (Serial Data Code Full Text) and base everything off of that home base.
> "The system of ..." as a Correct-Incorrect Intent Metaphor (legacy: CIsrm).
> The metaphors include metaphors that would help you get through the SMART training of same-different (e.g., hot vs. cold) and more-less (e.g., imagining "getting closer" to the ideal image, especially in a context where you are memorizing a recollection from a cue), hierarchy (e.g., clearly imagining the hierarchy of details)

If you have details in hierarchies, one hierarchy stained RED and one GREEN, the point is to learn arbitrary staining in the context of RED-GREEN ping-pong.

Principle of surprise >> on how surprising the difference between present and memory is, you should judge the quality of mental recall (affects all RFT relations earlier described)
